---
title: "Position: Explain to Question not to Justify"
authors: "Przemyslaw Biecek, Wojciech Samek"
collection: publications
permalink: /publication/2024-06-24-RED-XAI
hashtags: '<font color="red">#RED-XAI</font> <font color="blue">#BLUE-XAI</font>'
excerpt: "This position paper introduces a conceptual division in XAI between BLUE XAI (human/value-oriented) and RED XAI (model/validation-oriented), emphasizing the latter's critical yet underexplored role in ensuring AI safety. The authors advocate for intensified research in RED XAI to enable deeper model interrogation, bug detection, and knowledge extraction, outlining key challenges and opportunities in this emerging direction."
date: 2024-06-24
venue: 'International Conference on Machine Learning (ICML)'
paperurl: 'https://openreview.net/forum?id=ooikIHLHCs'
---

Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. We separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). This position paper argues that the area of RED XAI is currently under-explored, i.e., more methods for explainability are desperately needed to question models (e.g., extract knowledge from well-performing models as well as spotting and fixing bugs in faulty models), and the area of RED XAI hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.

