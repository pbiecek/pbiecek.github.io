---
title: "Interpreting CLIP with Hierarchical Sparse Autoencoders"
authors: "Vladimir Zaigrajew, Hubert Baniecki, Przemyslaw Biecek"
collection: publications
permalink: /publication/2025-06-24-MSAE-CLIP
hashtags: '<font color="#00214D">#Multimodal-XAI</font> <font color="#203354">#SAE</font>'
excerpt: 'This paper introduces Matryoshka SAE (MSAE), a hierarchical sparse autoencoder that optimizes both reconstruction quality and sparsity for interpreting large-scale vision-language models like CLIP. MSAE achieves a new state-of-the-art trade-off, enabling extraction of over 120 semantic concepts for tasks such as concept-based search and bias analysis, thus enhancing interpretability and control in multimodal systems.'
date: 2025-06-24
venue: 'International Conference on Machine Learning (ICML)'
paperurl: 'https://arxiv.org/abs/2502.20578'
---

Sparse autoencoders (SAEs) are useful for detecting and steering interpretable features in neural networks, with particular potential for understanding complex multimodal representations. Given their ability to uncover interpretable features, SAEs are particularly valuable for analyzing large-scale vision-language models (e.g., CLIP and SigLIP), which are fundamental building blocks in modern systems yet remain challenging to interpret and control. However, current SAE methods are limited by optimizing both reconstruction quality and sparsity simultaneously, as they rely on either activation suppression or rigid sparsity constraints. To this end, we introduce Matryoshka SAE (MSAE), a new architecture that learns hierarchical representations at multiple granularities simultaneously, enabling a direct optimization of both metrics without compromise. MSAE establishes a new state-of-the-art Pareto frontier between reconstruction quality and sparsity for CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of variance unexplained while maintaining ~80% sparsity. Finally, we demonstrate the utility of MSAE as a tool for interpreting and controlling CLIP by extracting over 120 semantic concepts from its representation to perform concept-based similarity search and bias analysis in downstream tasks like CelebA. We make the codebase available at this https URL.

